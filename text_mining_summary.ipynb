{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Text Mining <center>\n",
    "---\n",
    "## <div style=\"text-align: right\"> E-비즈니스학과 서창덕 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 마이닝의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **텍스트 마이닝**\n",
    "\n",
    "텍스트로부터 무언가 **high-quality information**(고수준의, 양질의 정보)를 뽑아내는 과정\n",
    "\n",
    "**pattern and trends**을 찾아라!\n",
    "\n",
    "**통계적 패턴 학습** → 숨어있는 패턴과 트렌드를 찾아냄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, **텍스트**(비정형 데이터) → **분석이 가능**한 정형 데이터로 변환 (이를 위해서 NLP(자연어 처리)를 비롯한 여러 분석 방법들을 사용) → 변환 후 **패턴**이나 **트렌드**를 찾아서 유용한 정보를 만들어 내는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트(하나의 문서, 문장)는 **가변**의 길이 → **일정한 길이의 벡터**로 변환(**숫자**)\n",
    "\n",
    "이렇게 변환하고 나면 머신러닝 기법을 사용하기 용이함.\n",
    "\n",
    "왜냐하면 **머신러닝**은 수치화된 어떠한 것들을 대상으로 작동하는 기법이기 때문에 **변환된 벡터**에 기법을 적용하는 것이 가능해짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 text minig을 통해서 구체적으로 할 수 있는 것들(목적)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- text classification(텍스트 분류)<br>\n",
    "    ex. 스팸 메일 분류\n",
    "- clustering\n",
    "- 감성 분석 (text classification의 한 분류)<br>\n",
    "    어떤 글이 어떤 주제에 대해서 우호적인지, 부정적인지 비교<br>\n",
    "    ex. 만약 영화 리뷰를 썼다면 이 영화 리뷰가 이 영화에 대해서 좋다고 말하고 있는건지, 나쁘다고 말하고 있는지 판단\n",
    "\n",
    "- document summarization\n",
    "- machine translation\n",
    "- prediction<br>\n",
    "⇒ 변환된 vector에 머신러닝(딥러닝) 기법을 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트 마이닝의 이해를 위한 기본요구지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리\n",
    "\n",
    "⇒ 텍스트라는 단어들의 **연속된 시퀀스**를 **수치화된 벡터**로 변환 \n",
    "\n",
    "- 통계학 & 선형대수\n",
    "    - 조건부 확률, 벡터, 선형 결합...\n",
    "- 머신러닝\n",
    "    - 회귀분석의 개념\n",
    "    - 머신러닝의 다양한 기법 (나이브 베이즈, Decision Tree 등...)\n",
    "- 딥러닝\n",
    "    - 딥러닝의 개념\n",
    "    - 딥러닝의 다양한 기법 (CNN, RNN 등...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 마이닝 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP(Natural Language Processing) 기본도구\n",
    "    - Tokenize, stemming, lemmatize\n",
    "    - Chunking\n",
    "    - BOW, TFIDF - **sparse represntation**\n",
    "<br>\n",
    "\n",
    "- 머신러닝(딥러닝)\n",
    "    - Naive Bayes, Logistic regression, Decsion tree, SVM\n",
    "    - Embedding(Word2Vec, Doc2Vec) - **dense representation**\n",
    "    - RNN(LSTM), Attention, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 마이닝 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- document = 텍스트 마이닝의 대상(하나의 문서, 하나의 문장, 책 한권)\n",
    "\n",
    "가장 먼저 Document → tokenize(단어 단위로 쪼개는 것)와 normalize(변형되어 쓰여진 단어들을 원래 원형으로 돌리는 것) 작업 진행\n",
    "\n",
    " \n",
    "\n",
    "    tokenize 예시 → she is my girlfriend → she, is, my, girlfriend\n",
    "\n",
    "    normalize 예시 → go, went는 의미상으로 같으나 그냥 두면 의미상으로 같다는 것을 알 수 없기 때문에 went → go(원형)으로 바꿔줌.\n",
    "\n",
    "이 결과 **Sequence of normalized words** 가 생김.\n",
    "\n",
    "ex. ['I', 'have', 'a', 'pencil'] \n",
    "\n",
    "순서가 의미가 있는 단어들의 sequence가 됨.\n",
    "\n",
    "여기까지가 자연어의 기본 처리임.\n",
    "\n",
    "이걸 가지고 다루는 세 가지 방법이 있음.\n",
    "\n",
    "    a) Fixed size vector **without** sequence info.\n",
    "\n",
    "        ['I', 'have', 'a', 'pencil']는 시퀀스가 의미가 분명히 있음에도 불구하고,\n",
    "\n",
    "        이 방법론에서는 ****순서를 무시하고 vector를 만듦.\n",
    "\n",
    "    b) Fixed size vector **with** sequence info.\n",
    "\n",
    "        순서(문맥) 정보를 포함하는 Fixed size vector로 바꿈.\n",
    "\n",
    "    c) Series of **Word Embedding** with sequence info.\n",
    "\n",
    "        순서(문맥) 정보를 유지하면서 Word Embedding된 vector들의 series\n",
    "\n",
    "        ['I', 'have', 'a', 'pencil']에서 'I'가 그냥 'I'로 남는 것이 아니라 어떠한 fixed size vector로 바뀌게 되고, 나머지도 마찬가지로 변환. 그리고 이것들의 시퀀스로 표현 \n",
    "\n",
    "        ['I', 'have', 'a', 'pencil'] → [Fixed size vector, Fixed size vector, Fixed size vector, Fixed size vector]\n",
    "\n",
    "⇒ a, b, c 각각에 따라 사용할 수 있는 방법론이 달라짐!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 텍스트 마이닝 적용분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document classification\n",
    "    - Sentiment analysis, calssification\n",
    "    \n",
    "- Document generation\n",
    "    - Q&A, summarization, translation\n",
    "    \n",
    "- Keyword extraction\n",
    "    - tagging/annotation\n",
    "    \n",
    "- Topic modeling\n",
    "    - LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 텍스트 마이닝 도구 - 파이썬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLTK\n",
    "    - 가장 많이 알려진 NLP 라이브러리(영어 대상으로 할 때 GOOD)\n",
    "    \n",
    "- Scikit Learn\n",
    "    - 머신러닝 라이브러리\n",
    "    - 기본적인 NLP, 다양한 텍스트 마이닝 관련 도구 지원\n",
    "- Gensim\n",
    "    - Word2Vec으로 유명\n",
    "    - sklearn과 마찬가지로 다양한 텍스트 관련 도구 지원\n",
    "- Keras\n",
    "    - RNN, seq2seq 등 딥러닝 위주의 라이브러리 제공\n",
    "    \n",
    "- **Pytorch**(최근 엄청 올라옴)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 텍스트 마이닝 기본 도구(NLP 중심)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목적: document, sentence 등을 **sparse vector**로 변환\n",
    "- Tokenize\n",
    "    - 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "- Text normalization\n",
    "    - 최소 단위를 표준화\n",
    "- POS-tagging\n",
    "    - 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "- Chunking\n",
    "    - POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말 모듬으로 다시 합치는 과정\n",
    "- BOW, TFIDF\n",
    "    - tokenized 결과를 이용하여 문서를 vector로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document -> sentence<br>\n",
    "sentence -> word<br>\n",
    "document -> word<br>\n",
    "\n",
    "- 영어 vs 한글\n",
    "    - 한글은 구조상 형태소(morpheme) 분석이 필요\n",
    "    - 복합명사, 조사, 어미 등을 분리해내는 작업이 필요\n",
    "    - 영어에 비해 어렵고 정확도 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "    ex. go, went에서 went -> go로\n",
    "    \n",
    "- Stemming(어간 추출)\n",
    "    - 의미가 아닌 규칙에 의한 변환\n",
    "    - 영어의 경우, Porter stemmer, Lancaster stemmer 등이 유명\n",
    "    \n",
    "- Lemmatization(표제어 추출)\n",
    "    - 사전을 이용하여 단어의 원형을 추출(의미 파악)\n",
    "    - 품사를 고려"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **최소단위**에 대해 **품사**를 결정하여 할당\n",
    "- 동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함\n",
    "- POS-tagging은 형태소 분석으로 번역되기도 하는데, 형태소 분석은 주어진 텍스트(원시말뭉치)를 형태소 단위로 나누는 작업을 포함하므로 앞의 tokenize, normalization 작업에 **품사 태깅**을 포함한 것으로 보는 것이 타당\n",
    "- 영어와 달리 한글 형태소 분석기는 토큰화, 정규화, POS-tagging이 한번에 모두 이루어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 말모듬\n",
    "- 명사구, 형용사구, 분사구와 같이 주어와 동사가 없는 두 단어 이상의 집합인 **구(phrase)**를 의미\n",
    "- 형태서 분석의 결과인 각 형태소들을 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) BOW, TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서를 **bag of words(BOW)**로 표현<br>\n",
    "원래 문장은 sequence of words임. 즉, bag of words로 표현 한다는 것은 시퀀스를 무시하겠다는 것이다. **어떤 단어가 있느냐가 중요**하지, 어느 위치에 있는지를 보지 않겠다는 의미\n",
    "<br>\n",
    "\n",
    "### **Vector Space Model**\n",
    "    - 단어가 쓰여진 순서 무시\n",
    "    - 모든 문서에 한번 이상 나타난 단어들에 대해 유(1)/ 무(0)로 문서를 표현\n",
    "    \n",
    "### **count vector**\n",
    "    - 단어의 유/무 대신 단어가 문서에 나타난 **횟수**로 표현\n",
    "    - count가 **weight**로 작용 \n",
    "    - 문제점:많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음\n",
    "    - ex. the, a,...(별 의미X)\n",
    "<br>\n",
    "\n",
    "### **TFIDF(Term Frequency - Inverse Document Frequency)**\n",
    "    - 단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "    - tf(d,f): 문서 d에 **단어 t가 나타난 횟수**, count vector와 동일, 로그 스케일등 다양한 변형이 있음.\n",
    "    - df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수 \n",
    "    - idf(t): df(t)의 역수를 바로 쓸 수도 있으나, 여러가지 이유로 로그스케일과 스무딩을 적용한 공식을 사용\n",
    "    - log(n/1+df(t)), n = 전체 문서 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Text classification with BOW/TFIDF\n",
    "- Naive Bayes\n",
    "- Logistic regression\n",
    "    - Ridge regression\n",
    "    - Lasso regression\n",
    "- Decision tree\n",
    "    - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (x1,...,xn)의 단어집합으로 이루어진 문서가 분류 Ck에 속할 확률\n",
    "\n",
    "**p(Ck | x1,...,xn)** = p(Ck, x1, ..., xn)<br>\n",
    "= p(Ck) * p(x1 | Ck) * p(x1 | Ck)...<br>\n",
    "= p(Ck) * i = 1~n까지 p(xi | Ck)를 다 곱해준 것\n",
    "- 만약 스팸 메일인지 아닌지를 분류하는 모델을 만든다면<br>\n",
    "    그 안에 쓰여진 단어를 보고 분류(스팸인 메일에는 주로 어떤 단어들이 사용되고, 스팸이 아닌 메일에는 어떤 단어들이 사용되는지)\n",
    "- (x1,...,xn) = 사용된 단어들\n",
    "- Ck = 스팸인지 아닌지\n",
    "- 즉, **메일에 사용된 어떤 단어들이 있을 때 이 문서가 스팸일 확률은 얼마인가**를 구하고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Logistic Regression\n",
    "- 분류를 위한 회귀분석\n",
    "    - 종속 변수 - 독립변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용\n",
    "    - 종속 변수 = **범주형** 데이터 대상, 입력 데이터가 주어졌을 때 해당 결과가 특정 분류로 나뉘기 때문에 일종의 분류 기법에 해당\n",
    "- 텍스트 마이닝에서의 문제\n",
    "    - 추정해야 할 계수가 vector의 크기(단어의 수)만큼 존재하므로, **과적합이 발생하기 쉽고** 많은 데이터 셋이 필요\n",
    "    - 그럼에도 잘 작동하는 편\n",
    "    - **정규화(regulation)**을 이용해 과적합 해결 노력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Ridge regression\n",
    "   - 목적 함수에 추정할 계수(파라미터)에 대한 L2 norm(규제항)을 추가하여 모형의 **과적합**을 방지\n",
    "   - L2 norm을 써서 릿지 회귀를 쓰면 일반적으로 성능이 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Lasso regression\n",
    "   - L1 norm을 사용\n",
    "   - 성능 향상보다는 **실제로 영향을 거의 미치지 않는 단어들을 제외**시키는 것에 목적\n",
    "   - 주로 어떤 단어가 영향을 미쳤는지 알고 싶다면 라쏘를 해보면 좋음\n",
    "   - **feature selection** 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Decision Tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
